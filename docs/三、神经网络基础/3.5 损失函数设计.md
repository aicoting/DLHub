# 损失函数设计
在神经网络的训练过程中，损失函数（Loss Function） 决定了模型优化的方向。没有损失函数，神经网络就不知道该往哪里调整参数，也就无法真正学会任务。损失函数的设计不仅关乎模型能不能收敛，更直接影响最终的性能。

## 损失函数的作用
损失函数的核心任务是：衡量预测值和真实值之间的差距。
- 如果预测结果和真实标签非常接近，损失就会很小；
- 如果预测偏差很大，损失就会很大。

训练神经网络的目标，就是不断最小化损失函数，让模型的预测越来越准确。
## 常见的损失函数
### 均方误差（MSE）
$$L = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$$
- $$n$$：样本数量
- $$y_i$$：第 i 个样本的真实值（标签）
- $$\hat{y}_i$$：第 i 个样本的预测值
- $$L$$：最终的损失值

MSE 是回归问题最常用的损失函数，它衡量预测值和真实值的平均平方差。

数值越大，说明偏差越大；数值越小，说明模型预测得越接近真实结果。但因为平方会放大误差，MSE 对异常值比较敏感。
### 交叉熵损失
$$L = -\sum_{i=1}^n y_i \log(\hat{y}_i)$$
- $$n$$：类别数量
- $$y_i$$：第 $$i$$ 个类别的真实分布（通常是 one-hot 向量）
- $$\hat{y}_i$$：模型对第 $$i$$ 个类别预测的概率
- $$L$$：损失值

交叉熵刻画的是两个概率分布之间的差异，一个是真实分布 $$y$$，一个是模型预测的分布 $$\hat{y}$$。

当模型预测的概率和真实标签一致时，交叉熵最小。它是分类任务的标准配置，和 Softmax 搭配使用效果最好。

### Dice Loss
$$L = 1 - \frac{2|P \cap G|}{|P| + |G|}$$
- $$P$$：模型预测的前景像素集合
- $$G$$：真实标注的前景像素集合
- $$|P|$$：预测前景的像素总数
- $$|G|$$：真实前景的像素总数
- $$|P \cap G|$$：预测和真实的重叠像素数量

Dice Loss 常用于图像分割，尤其是在医学图像等类别极不平衡的任务中。它直接度量预测区域和真实区域的重叠程度。Dice 系数越大说明重叠越多，损失就越小，模型效果也就越好。

### 对比损失

$$L = y \cdot d^2 + (1-y) \cdot \max(0, m-d)^2$$
- $$y$$：样本对的标签，若两个样本属于同一类则 $$y=1$$，否则 $$y=0$$
- $$d$$：两个样本在特征空间的欧式距离
- $$m$$：间隔（margin），用于控制负样本之间的最小距离
- $$L$$：损失值

对比损失常用于度量学习，比如人脸验证或语音验证。它的目标是让同类样本的特征距离更近（收敛到一起），不同类样本的特征距离更远（至少要大于 margin）。这样训练出来的特征空间，就能更好地区分不同类别。


在实际应用中，损失函数往往不是现成可用的，而是需要结合任务特点来设计：
- 分类任务：常用交叉熵，但在类别极度不平衡时，可以加权或使用 Focal Loss。
- 回归任务：MSE 是基础，如果异常值影响过大，可以用 MAE 或 Huber Loss。
- 检测/分割任务：常见做法是交叉熵 + Dice Loss 组合，兼顾整体和局部精度。
- 表征学习：需要对比损失、Triplet Loss 或 InfoNCE Loss 来优化特征空间的分布。
- 多任务学习：多个损失函数加权组合（例如目标检测中的分类 + 回归）。

损失函数是神经网络的“灵魂”。它决定了模型到底在优化什么，也影响了最终能学到什么。一个好的损失函数设计，往往比单纯加深网络、增加参数更有效。

最新的文章都在公众号更新，别忘记关注哦！！！如果想要加入技术群聊，扫描下方二维码回复【加群】即可。
<img src="./imgs/coting_qrcode.png" alt="" loading="lazy" decoding="async" />