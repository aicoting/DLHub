# 神经元模型
在神经网络里，最核心的计算单元就是神经元（Neuron），它就像人脑里的一颗神经细胞，负责接收输入、加工处理，然后把结果传递下去。整个神经网络，就是由成千上万甚至数十亿个神经元层层堆叠起来的。理解神经元模型，就是理解神经网络的第一步。
## 神经元的基本结构
一个典型的人工神经元包含以下几个部分：
1. 输入（Input）：神经元会接收多个输入信号，比如特征向量中的每个维度。
2. 权重（Weight）：每个输入都会乘上一个对应的权重，权重的大小决定了该输入在神经元决策中的重要性。
3. 加权求和（Weighted Sum）：所有输入和权重相乘，再加上一个偏置（Bias），得到一个综合值。
4. 激活函数（Activation Function）：加权求和的结果会通过一个非线性函数，从而引入非线性特性，让网络能够处理复杂的模式。
5. 输出（Output）：经过激活函数的结果会作为输出传递给下一层神经元。

公式可以写成： $$y = f\left(\sum_{i=1}^n w_i x_i + b\right)$$

其中：
- $$x_i$$：第 i 个输入
- $$w_i$$：第 i 个权重
- $$b$$：偏置
- $$f$$：激活函数
- $$y$$：输出

## 激活函数的作用
如果没有激活函数，神经元的输出就是输入的线性组合，整个网络也只是一个大的线性函数，没法解决复杂问题。激活函数的引入，使神经元具备了非线性表达能力。常见的激活函数包括：
- Sigmoid：输出范围在 $$(0,1)$$，早期常用于二分类问题，但容易梯度消失。
- Tanh：输出范围在 $$(-1,1)$$，相对 Sigmoid 更对称一些，但仍有梯度消失问题。
- ReLU：目前最常用的激活函数，公式简单 $$f(x)=\max(0,x)$$，收敛速度快，但有“死亡ReLU”现象。
- Leaky ReLU、GELU 等：是 ReLU 的改进版本。

可以把神经元类比成一个加权投票器：
- 每个输入就是一个投票，权重决定了投票的分量，偏置相当于先入为主的倾向。
- 当投票结果达到一定阈值时（激活函数处理后），神经元就会点亮，输出一个信号。
这种机制的好处是，可以把简单的输入特征组合成复杂的模式。比如在图像识别中，底层神经元可能只会识别边缘这种简单特征，而高层神经元则可能识别出眼睛、鼻子，再往上组合出人脸。

神经元的权重和偏置，并不是固定的，而是在训练过程中不断调整的。

具体来说：
1. 前向传播：输入数据经过神经元层层计算，得到输出结果。
2. 反向传播：计算预测结果和真实标签之间的误差，并通过链式法则把误差分解，逐层传回。
3. 参数更新：根据梯度下降（或其他优化算法），更新每个神经元的权重和偏置。

经过大量数据的训练，每个神经元就会逐渐学会在不同场景下该如何激活，最终形成强大的模式识别能力。

神经元模型看似简单，就是加权求和 + 激活函数，但正是这种小小的单元，当数量足够多、连接方式足够复杂时，才造就了今天的深度学习。它就像是搭建摩天大楼的一块砖，看似微不足道，却是不可或缺的基石。


最新的文章都在公众号更新，别忘记关注哦！！！如果想要加入技术群聊，扫描下方二维码回复【加群】即可。
<img src="./imgs/coting_qrcode.png" alt="" loading="lazy" decoding="async" />
