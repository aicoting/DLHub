---
lock: true
---
# 正则化（Dropout）
在深度学习训练中，我们经常遇到一个老生常谈的问题——过拟合（Overfitting）。训练集上精度高得离谱，测试集却惨不忍睹。原因很简单：模型记住了训练数据的细枝末节，却没学到真正有用的规律。

那怎么办？我们得想办法让模型在学习时不过度依赖某些特征——这就是 Dropout 登场的地方。

## Dropout 是什么？
Dropout 是一种简单却非常有效的正则化方法，由 Hinton 在 2014 年提出。

它的核心思想一句话概括就是：
在训练过程中，随机让一部分神经元休息，让网络每次都在不同的子网络上训练。

通俗点说，Dropout 就像是在课堂上随机点名，让不同的学生回答问题，防止某几个学霸包办全部回答，逼着学渣也得学点东西。
## Dropout 的原理
在标准的神经网络中，每一层的输出都是通过前一层的全部神经元计算得到的，这会导致某些神经元之间出现强依赖关系（co-adaptation）。

Dropout 的做法是：在每次前向传播时，以概率 ( $$p$$ ) 将部分神经元的输出置为 0，不参与当前批次的计算。

例如：

假设某一层有 100 个神经元，Dropout 率 ( $$p = 0.5$$ )，那这一轮训练中只有大约 50 个神经元会被激活，剩下的直接休假。
数学上可以表示为：

$$h_i' = \begin{cases} h_i / (1-p), & \text{if neuron i is kept} \\\ 0 & \text{if neuron i is dropped} \end{cases}$$

其中 ( $$1-p$$ ) 是一个缩放系数，用来保证训练和推理阶段的期望输出一致。
## 训练与推理的区别
- 训练阶段（Training）：
 随机丢弃一部分神经元，让网络在不同的子结构上学习特征。
- 推理阶段（Inference）：
 不再丢弃任何神经元，而是将每个神经元的输出按保留概率 ( $$(1-p)$$ ) 缩放。

这样做的目的是让模型在训练时更具多样性，而在预测时汇总所有子网络的学习结果。可以把它理解为一种集成学习（Ensemble Learning）思想的高效实现。
## Dropout 的优点
1. 有效防止过拟合：
 随机失活让模型无法死记硬背训练样本，提升泛化能力。
2. 相当于模型集成：
 每次训练的子网络不同，最终的模型相当于多个网络的平均效果。
3. 实现简单：
 一行代码搞定，在现代框架（PyTorch、TensorFlow）中都是内置模块。
## Dropout 的使用技巧
- 一般在 全连接层 中使用效果最好；
- 对 卷积层，Dropout 作用相对有限，通常使用较低的比例（如 0.2）；
- 常见的 Dropout 概率：
  - 0.5 → 全连接层
  - 0.2～0.3 → 卷积层
- 与 BatchNorm 同时使用时，建议在 BatchNorm 之后 再加 Dropout。
## PyTorch 实现示例
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 定义一个简单的全连接网络，带Dropout
class DropoutNet(nn.Module):
    def __init__(self, input_size=10, hidden_size=10, dropout_p=0.5):
        super(DropoutNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.dropout = nn.Dropout(p=dropout_p)
        self.fc2 = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # 训练阶段随机丢弃
        x = self.fc2(x)
        return x

# 创建网络实例
model = DropoutNet(input_size=10, hidden_size=10, dropout_p=0.5)

# 模拟一批输入数据
x = torch.ones((1, 10))

# 训练模式下前向传播
model.train()  # 激活Dropout
y_train = model(x).detach().numpy()
print("训练模式下输出:", y_train)

# 推理模式下前向传播
model.eval()  # 关闭Dropout
y_eval = model(x).detach().numpy()
print("推理模式下输出:", y_eval)
```
在这段代码中，每次前向传播，Dropout($$p=0.5$$) 会随机让一半神经元休眠，从而提升模型泛化。
## 总结
Dropout 的精妙之处在于它的“随机性”——让网络在学习过程中不断打乱组合，迫使模型更广泛地利用特征，从而学得更稳、更通用。

可以说，它是深度学习历史上最简单但影响力极大的正则化方法之一。如今在各类模型（尤其是 MLP、Transformer 编码器等）中仍被广泛使用。

最新的文章都在公众号更新，别忘记关注哦！！！如果想要加入技术群聊，扫描下方二维码回复【加群】即可。
<img src="./imgs/coting_qrcode.png" alt="" loading="lazy" decoding="async" />