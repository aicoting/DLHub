---
lock: true
---
# 标签平滑（Label Smoothing）
训练深度学习模型时，我们常常希望它能对正确的类别输出尽可能高的置信度。但如果太高了，比如 0.9999，那模型就有点自恋了。它不仅会过拟合，还容易在面对新样本时信心满满地出错。

Label Smoothing（标签平滑）就是为了解决这个问题，让模型学会 谦虚一点。

我们在分类任务中通常会用 One-hot 标签，比如对于 3 类分类问题：

|类别|One-hot 标签|
|--|--|
|类别1|	[1, 0, 0]|
|类别2|	[0, 1, 0]|
|类别3|	[0, 0, 1]|

这没问题，但这种标签对模型来说太极端了， 它告诉模型——除了正确类以外，其他类别概率必须是 0。

现实中呢？数据本身可能有噪声，标注也未必完美，甚至某些类别之间还存在语义上的相似性。比如图像识别里，“哈士奇”和“阿拉斯加”就容易搞混。如果硬逼模型输出 100% 的确定性，就会让模型对错误过度自信。

于是我们希望给正确类别稍微降点温，给错误类别留一点点空间。

这就是 Label Smoothing 的思想。
## 公式原理
假设分类任务有  $K$  个类别，真实标签是 one-hot 编码的向量 $$y \in {0,1}^K$$

标签平滑后的目标分布是：
 $$y_i^{LS} =\begin{cases} 1 - \varepsilon, & \text{if } i = \text{true class} \\ \frac{\varepsilon}{K-1}, & \text{otherwise} \end{cases}$$

其中  $$\varepsilon$$  是平滑系数（通常取 0.1 或 0.05）。

举个例子：

对于 3 分类问题、平滑系数  $$\varepsilon$$ = 0.1 ：

原标签： $$y = [1, 0, 0]$$
平滑后： $$y^{LS} = [0.9, 0.05, 0.05]$$

这意味着模型在训练时不会再被强迫输出完美的1，而是输出一个温和一点的置信分布。

## 对损失函数的影响
在交叉熵损失中，原始形式是： $$\mathcal{L} = - \sum_{i=1}^{K} y_i \log(p_i)$$

加入 Label Smoothing 后，目标标签变成  $$y_i^{LS}$$ ：$$\mathcal{L}_{LS} = - \sum_{i=1}^{K} y_i^{LS} \log(p_i)$$

相比原来的 one-hot，平滑后的损失不会过度惩罚非目标类别的预测，从而让模型输出更平衡、更稳健的概率分布。
## 为什么 Label Smoothing 有用？
**1. 防止过拟合**

当模型被要求输出 100% 确信时，它容易把噪声当真，从而过拟合。 Label Smoothing 让模型保持一点不确定性，提升泛化能力。

**2. 改善校准性（Calibration）**

校准性指模型预测的概率是否符合真实分布。过度自信的模型往往校准性差。加入平滑后，模型预测的置信度更贴近真实概率。

**3. 提升鲁棒性**

在对抗样本或分布变化的数据上，模型更稳，不容易被骗。

## 代码示例
我们来看一个简单的 PyTorch 示例，对比一下普通交叉熵与 Label Smoothing 的区别。
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt

# 模拟一个 batch，4 个样本，3 类
logits = torch.tensor([[2.5, 0.3, 0.2],
                       [0.1, 2.0, 0.1],
                       [1.2, 0.9, 0.3],
                       [0.5, 0.5, 1.8]])

targets = torch.tensor([0, 1, 1, 2])  # 真实标签

# 普通交叉熵
loss_ce = nn.CrossEntropyLoss()(logits, targets)

# 含标签平滑的交叉熵
loss_ls = nn.CrossEntropyLoss(label_smoothing=0.1)(logits, targets)

print("普通交叉熵损失:", loss_ce.item())
print("标签平滑交叉熵损失:", loss_ls.item())

# 可视化预测分布
probs = F.softmax(logits, dim=-1).detach().numpy()
plt.bar(range(3), probs[0], color=['red','gray','gray'])
plt.title("Example 1 Prediction Distribution")
plt.xlabel("Class index")
plt.ylabel("Predicted Probability")
plt.show()
```
这个结果按理来说是普通交叉熵的损失更大，因为它惩罚非目标类非常重；Label Smoothing 的损失更小，分布也更平滑，但是实际上相反，这是和数据量的大小有关系。

## 实践经验与注意事项
**1. 平滑系数的选择**
- 一般在 0.05~0.2 之间；
- 平滑太大会损失模型区分能力；
- 平滑太小又起不到作用。

**2. 不适合回归任务**

Label Smoothing 主要用于分类任务（尤其是多分类），在回归场景无意义。

**3. 在大型模型中尤其常见**

比如 Transformer、Vision Transformer、BERT 等模型的训练中，Label Smoothing 已经是默认配置之一（Transformer 论文里直接用了 $$ε=0.1$$）。

总结一下，Label Smoothing 就像给模型加了一点人性， 它不会让模型绝对自信，而是学会即便我很确定，也要留一点余地。在如今的数据驱动世界里，学会不盲信绝对真理，反而能走得更远。这就是 Label Smoothing 的哲学。

最新的文章都在公众号更新，别忘记关注哦！！！如果想要加入技术群聊，扫描下方二维码回复【加群】即可。
<img src="./imgs/coting_qrcode.png" alt="" loading="lazy" decoding="async" />