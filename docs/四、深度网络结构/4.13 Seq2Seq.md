---
lock: true
---
# Seq2Seq 模型
在自然语言处理（NLP）、语音识别和机器翻译等任务中，我们经常需要将一个序列映射为另一个序列。例如，将一句英语翻译成法语，或将一段语音转写为文本。

为了解决这类 输入与输出序列长度可能不同 的问题，研究者提出了 Seq2Seq（Sequence-to-Sequence）模型，它通常由 编码器（Encoder） 和 解码器（Decoder） 两部分构成。

Seq2Seq 最早由 Sutskever 等人在 2014 年提出，广泛应用于 机器翻译、文本摘要、对话系统 等领域，并为后续的 Attention 机制与 Transformer 奠定了基础。

## Seq2Seq 的核心结构
<img src="./imgs/seq2seq.png" alt="" loading="lazy" decoding="async" />

Seq2Seq 的基本框架包含两个部分：

**1. 编码器（Encoder）**
  - 通常由 RNN（如 LSTM/GRU）构成。
  - 输入序列 $$(x_1, x_2, \dots, x_T)$$经过编码器后，得到最后的隐藏状态向量 $$h_T$$。
  - 该向量作为整个输入序列的语义表示，传递给解码器。

**2. 解码器（Decoder）**
  - 也是一个 RNN（如 LSTM/GRU）。
  - 以编码器的语义向量 $$h_T$$ 作为初始状态，逐步生成目标序列 $$(y_1, y_2, \dots, y_{T'})$$。
  - 每一步解码器的输入不仅依赖于前一时刻的预测结果，还依赖于隐藏状态。

整体流程： $$X = (x_1, \dots, x_T) \quad \xrightarrow{\text{Encoder}} \quad h_T \quad \xrightarrow{\text{Decoder}} \quad Y = (y_1, \dots, y_{T'})$$

## Seq2Seq 的训练过程
Seq2Seq 的训练通常采用 教师强制（Teacher Forcing）：
- 在训练时，解码器每一步的输入使用 真实的目标序列 $$y_{t-1}$$，而不是模型预测的结果。
- 这样可以加速收敛，减少误差传播。

损失函数通常选择 交叉熵损失（Cross-Entropy Loss），用于衡量预测序列与真实目标序列的差异。

虽然 Seq2Seq 框架简单有效，但也存在一些局限：

**1. 信息瓶颈问题**
  - 编码器最后的隐藏状态向量 $$h_T$$ 需要压缩整个输入序列的信息。
  - 对于长序列，信息会丢失，导致性能下降。

**2. 长距离依赖问题**
  - 尽管使用了 LSTM/GRU，但在超长序列任务中仍然存在建模不足。

为了解决这些问题，研究者引入了 注意力机制（Attention Mechanism），允许解码器在生成输出时动态关注输入序列中的不同部分。这一改进最终催生了 Transformer 架构。

## 代码示例
下面给出一个简化版 Seq2Seq（基于 LSTM 的 Encoder-Decoder） 的 PyTorch 实现：
```python
import torch
import torch.nn as nn

# 编码器
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Encoder, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)

    def forward(self, x):
        outputs, (hidden, cell) = self.lstm(x)
        return hidden, cell  # 返回最后的隐藏状态和记忆单元

# 解码器
class Decoder(nn.Module):
    def __init__(self, output_size, hidden_size):
        super(Decoder, self).__init__()
        self.lstm = nn.LSTM(output_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden, cell):
        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))
        predictions = self.fc(outputs)
        return predictions, hidden, cell

# Seq2Seq 模型
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        hidden, cell = self.encoder(src)
        outputs = []
        input = trg[:, 0:1, :]  # 第一个输入 (起始符号)

        for t in range(1, trg.size(1)):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs.append(output)
            # 是否使用教师强制
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            input = trg[:, t:t+1, :] if teacher_force else output
        return torch.cat(outputs, dim=1)

# 示例
encoder = Encoder(input_size=10, hidden_size=20)
decoder = Decoder(output_size=10, hidden_size=20)
model = Seq2Seq(encoder, decoder)

src = torch.randn(5, 7, 10)  # (batch=5, seq_len=7, input_size=10)
trg = torch.randn(5, 9, 10)  # (batch=5, seq_len=9, output_size=10)
out = model(src, trg)
print(out.shape)  # 输出: (5, 8, 10)
```
Seq2Seq 是 序列建模的经典架构，通过 Encoder-Decoder 结构实现了输入序列到输出序列的映射。在机器翻译等任务中取得了突破性成果。

然而，Seq2Seq 受制于 信息瓶颈，在长序列任务中表现不足。随着 注意力机制（Attention） 的引入，Seq2Seq 演化出 Seq2Seq + Attention，并最终发展到 Transformer，开启了深度学习在 NLP 的新时代。

最新的文章都在公众号更新，别忘记关注哦！！！如果想要加入技术群聊，扫描下方二维码回复【加群】即可。
<img src="./imgs/coting_qrcode.png" alt="" loading="lazy" decoding="async" />